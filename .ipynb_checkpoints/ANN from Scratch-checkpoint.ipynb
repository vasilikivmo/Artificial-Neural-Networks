{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create an ANN in Python following OOP framework.\n",
    "This means, we will build our ANN as a class we can build objects inheriting its methods.  \n",
    "\n",
    "Remember, there is no magic, just intelligence! Read the detailed theory in this document LINKS, we will use the same notation in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ecuations we need to know behore hand can be founded in the document in this folder: ANN Equations. We will reference those equation in the code, therefore, I recommend you to split you screen and follow both together.  \n",
    "We will be following the next scheme:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Images/ANN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define the layers we are going to have and the neurons in each layer\n",
    "        self.inputLayerSize = x_train.shape[1]\n",
    "        self.hiddenLayerSize = 3\n",
    "        self.outputLayerSize = 1 \n",
    "\n",
    "        # Weights initialization \n",
    "        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Propagate inputs through neurons\n",
    "        self.z2 = np.dot(X, self.W1)       # Eq. 1\n",
    "        self.a2 = self.sigmoid(self.z2)    # Eq. 2\n",
    "        self.z3 = np.dot(self.a2, self.W2) # Eq. 3\n",
    "        yHat = self.sigmoid(self.z3)       # Eq. 4\n",
    "        return yHat\n",
    "\n",
    "    def sigmoid(self, z):   \n",
    "        # Sigmoid activaction function\n",
    "        return 1 / (1+np.exp(-z))\n",
    "\n",
    "    def dSigmoid(self, z):\n",
    "        # Derivative of sigmoid function\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "\n",
    "    ''' You can also try with different activation function and see how it performs!\n",
    "    def relu(self, z):\n",
    "        # Rectifier Linear Unit activaction funciton\n",
    "        return np.max(0, z)\n",
    "\n",
    "    def dRelu(self, z):\n",
    "        # Derivative of sigmoid function\n",
    "        if z > 0: return 1\n",
    "        else: return 0\n",
    "    '''    \n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        # Compute the Cost Function using weights already stored in class\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        J = np.sum((y - self.yHat)**2) / X.shape[0] # Eq. 6\n",
    "        return J     \n",
    "\n",
    "    def dCostFunction(self, X, y):\n",
    "        # Compute derivative with respect to W1 and W2 \n",
    "        self.yHat = self.forward(X)\n",
    "\n",
    "        delta3 = np.multiply(-(y-self.yHat), self.dSigmoid(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3) / X.shape[0] # Eq. 7\n",
    "\n",
    "        delta2 = np.dot(delta3, self.W2.T) * self.dSigmoid(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2) / X.shape[0]    # Eq. 8\n",
    "\n",
    "        return dJdW1, dJdW2\n",
    "\n",
    "    def getWeights(self):\n",
    "        # Get W1 and W2\n",
    "        return self.W1, self.W2\n",
    "    \n",
    "    def setWeights(self, W1, W2):\n",
    "        # Update the weights W1 and W2\n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.dCostFunction(X, y)\n",
    "        return dJdW1, dJdW2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our Training Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class trainer(object):\n",
    "     \n",
    "    def __init__(self, N):\n",
    "        # Reference the Neural Network that will be trained:\n",
    "        self.N = N\n",
    "\n",
    "    def callbackF(self, W1, W2):\n",
    "        # Callback function to track the J as we train the NN\n",
    "        self.N.setWeights(W1, W2)                                      # Set the new parameters coming from training\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))             # Insert new value of Cost to the Cost vector\n",
    "        self.testJ.append(self.N.costFunction(self.testX, self.testY)) # Insert new value of Test-Cost to the Test-Cost vector\n",
    "\n",
    "    def updateWeigths(self, W1, W2, grad1, grad2):\n",
    "        W1 = W1 - self.learning_rate * grad1\n",
    "        W2 = W2 - self.learning_rate * grad2\n",
    "        return W1, W2\n",
    "        \n",
    "    def train(self, trainX, trainY, testX, testY, epochs, learning_rate = 0.01):\n",
    "        # Make internal variables for callback function\n",
    "        self.X = trainX\n",
    "        self.y = trainY\n",
    "        self.testX = testX\n",
    "        self.testY = testY\n",
    "        self.epochs = range(epochs)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        #Make empty list to store the costs:\n",
    "        self.J     = []\n",
    "        self.testJ = []\n",
    "\n",
    "        for epoch in self.epochs:\n",
    "            cost = self.N.costFunction(self.X, self.y)\n",
    "            grad1, grad2 = self.N.computeGradients(self.X, self.y)\n",
    "            W1, W2 = self.N.getWeights()\n",
    "            W1, W2 = self.updateWeigths(W1, W2, grad1, grad2)            \n",
    "            # Update the weights\n",
    "            self.N.setWeights(W1, W2)\n",
    "            self.callbackF(W1, W2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.287613</td>\n",
       "      <td>11.843398</td>\n",
       "      <td>-23.111000</td>\n",
       "      <td>-26.784964</td>\n",
       "      <td>35.887831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-12.942548</td>\n",
       "      <td>-9.288027</td>\n",
       "      <td>-29.954254</td>\n",
       "      <td>36.694911</td>\n",
       "      <td>-33.618653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-11.743498</td>\n",
       "      <td>13.198859</td>\n",
       "      <td>17.154367</td>\n",
       "      <td>-9.360629</td>\n",
       "      <td>51.557494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.619156</td>\n",
       "      <td>-22.496186</td>\n",
       "      <td>-17.912477</td>\n",
       "      <td>36.132983</td>\n",
       "      <td>32.243452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.606882</td>\n",
       "      <td>-22.517781</td>\n",
       "      <td>23.364357</td>\n",
       "      <td>-3.593449</td>\n",
       "      <td>-24.734984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4\n",
       "0   0.287613  11.843398 -23.111000 -26.784964  35.887831\n",
       "1 -12.942548  -9.288027 -29.954254  36.694911 -33.618653\n",
       "2 -11.743498  13.198859  17.154367  -9.360629  51.557494\n",
       "3   1.619156 -22.496186 -17.912477  36.132983  32.243452\n",
       "4   6.606882 -22.517781  23.364357  -3.593449 -24.734984"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = np.random.randn(100, 5)*30\n",
    "df_dataset = pd.DataFrame(dataset)\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 18879.7238798 ,  38543.80025093,   1664.30263516,  25498.57575526,\n",
       "        -1362.18909605])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(dataset)\n",
    "Y = X[:,1] + 2*X[:,2] + 56*X[:,4] - X[:,2]*np.power(X[:,3], 2) + np.log(np.power(X[:,1], 100)) + 50*np.random.randn(100)\n",
    "Y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22330444,  0.2638671 ,  0.18779282,  0.23695768,  0.18154983,\n",
       "        0.20482247,  0.18288122,  0.13844135,  0.18101974,  0.1846012 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalization:\n",
    "scalatorX = preprocessing.MinMaxScaler()\n",
    "\n",
    "def scale(vector):\n",
    "    # Function to scale a vector to range [0 1]\n",
    "    maxs = np.max(vector)\n",
    "    mins = np.min(vector)\n",
    "    scaling = np.zeros(len(vector))\n",
    "    \n",
    "    for i in range(0, len(vector)):\n",
    "        scaling[i] = (vector[i] - mins) / (maxs - mins)\n",
    "    return scaling\n",
    "    \n",
    "def unScale(scal, unscal):\n",
    "    # Unscale function to a vector giving the scaled and an unscaled reference vector\n",
    "    maxs = np.max(unscal)\n",
    "    mins = np.min(unscal)\n",
    "    unscaling = np.zeros(len(scal))\n",
    "    \n",
    "    for i in range(0, len(scal)):    \n",
    "        unscaling[i] = scal[i] * (maxs-mins) + mins\n",
    "    return unscaling\n",
    "\n",
    "x = scalatorX.fit_transform(X)\n",
    "y = scale(Y)\n",
    "# y = scalatorY.fit_transform(Yreshape(-1, 1))\n",
    "\n",
    "# Normalization of test data:\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (70, 5)\n",
      "x_test: (30, 5)\n",
      "y_train: (70, 1)\n",
      "y_test: (30, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [ 0.17994662],\n",
       "       [ 0.17904877],\n",
       "       [ 0.20482247],\n",
       "       [ 0.13782762],\n",
       "       [ 0.18860265],\n",
       "       [ 0.18621815],\n",
       "       [ 0.1854796 ],\n",
       "       [ 0.1232883 ],\n",
       "       [ 0.21130993]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split as split\n",
    "x_train, x_test, y_train, y_test = split(X, y, test_size=0.3, random_state=2017)\n",
    "y_train = y_train.reshape((-1,1))\n",
    "y_test = y_test.reshape((-1,1))\n",
    "print('x_train:', x_train.shape)\n",
    "print('x_test:',  x_test.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('y_test:',  y_test.shape)\n",
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX: (4, 2)\n",
      "testX: (4, 2)\n",
      "trainY: (4, 1)\n",
      "testY: (4, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 70.],\n",
       "       [ 89.],\n",
       "       [ 85.],\n",
       "       [ 75.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX = np.array(([3,5], [5,1], [10,2], [6,1.5]), dtype=float)\n",
    "trainY = np.array(([75], [82], [93], [70]), dtype=float)\n",
    "\n",
    "# Testing Data:\n",
    "testX = np.array(([4, 5.5], [4.5,1], [9,2.5], [6, 2]), dtype=float)\n",
    "testY = np.array(([70], [89], [85], [75]), dtype=float)\n",
    "\n",
    "print('trainX:', trainX.shape)\n",
    "print('testX:',  testX.shape)\n",
    "print('trainY:', trainY.shape)\n",
    "print('testY:',  testY.shape)\n",
    "testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NN = Neural_Network() # Create an object NN \n",
    "T = trainer(NN)       # Create an object trainer for that NN created\n",
    "T.train(x_train, y_train, x_test, y_test, 200) # Pass data and epochs to the training method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ff8c400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(T.J, 'b-', label='Train set Loss')\n",
    "plt.plot(T.testJ, 'r-', label='Test set Loss')\n",
    "plt.grid(1)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it takes a lot of epochs to reduce the error (even we are in a range between 4-6%)  \n",
    "This can be improved if we declare a batch size for the inputs!   \n",
    "This means, that all the inputs in a batch are going to be computed, and then we will average the error and update the weights. This way, the weights are updated several times inside 1 epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class trainer(object):\n",
    "     \n",
    "    def __init__(self, N):\n",
    "        # Reference the Neural Network that will be trained:\n",
    "        self.N = N\n",
    "\n",
    "    def callbackF(self, W1, W2):\n",
    "        # Callback function to track the J as we train the NN\n",
    "        self.N.setWeights(W1, W2)                                      # Set the new parameters coming from training\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))             # Insert new value of Cost to the Cost vector\n",
    "        self.testJ.append(self.N.costFunction(self.testX, self.testY)) # Insert new value of Test-Cost to the Test-Cost vector\n",
    "\n",
    "    def updateWeigths(self, W1, W2, grad1, grad2):\n",
    "        W1 = W1 - self.learning_rate * grad1\n",
    "        W2 = W2 - self.learning_rate * grad2\n",
    "        return W1, W2\n",
    "    \n",
    "    def bucketize(vector, bucket_size):\n",
    "        x = \n",
    "        \n",
    "    def train(self, trainX, trainY, testX, testY, epochs, batch_size, learning_rate=0.01):\n",
    "        # Make internal variables for callback function\n",
    "        self.epochs = range(epochs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.X = trainX\n",
    "        self.y = trainY\n",
    "        self.testX = testX\n",
    "        self.testY = testY\n",
    "        \n",
    "\n",
    "        #Make empty list to store the costs:\n",
    "        self.J     = []\n",
    "        self.testJ = []\n",
    "\n",
    "        for epoch in self.epochs:\n",
    "            cost = self.N.costFunction(self.X, self.y)\n",
    "            grad1, grad2 = self.N.computeGradients(self.X, self.y)\n",
    "            W1, W2 = self.N.getWeights()\n",
    "            W1, W2 = self.updateWeigths(W1, W2, grad1, grad2)            \n",
    "            # Update the weights\n",
    "            self.N.setWeights(W1, W2)\n",
    "            self.callbackF(W1, W2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization -- Solve overfitting problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a simple technique that penalize complex model, helping us to avoid overfitting. \n",
    "\n",
    "Note how we are going to insert a little term into our cost function, and therefore, in its derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural_Network_Regularized(object):\n",
    "    \n",
    "    def __init__(self, Lambda=0):\n",
    "        # Define Hyperparamethers\n",
    "        self.inputLayerSize = 2  # trainX.shape[1] = 2\n",
    "        self.hiddenLayerSize = 3\n",
    "        self.outputLayerSize = 1 \n",
    "\n",
    "        # Weights initialization \n",
    "        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)  \n",
    "        \n",
    "        # Regularization Parameter:\n",
    "        self.Lambda = Lambda\n",
    "            \n",
    "    def forward(self, X):\n",
    "        # Propagate inputs through neurons\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3)\n",
    "        return yHat\n",
    "\n",
    "    def sigmoid(self, z):   \n",
    "        # Sigmoid activaction function\n",
    "        return 1 / (1+np.exp(-z))\n",
    "\n",
    "    def dSigmoid(self, z):\n",
    "        # Derivative of sigmoid function\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def relu(self, z):\n",
    "        # Rectifier Linear Unit activaction funciton\n",
    "        return np.max(0, z)\n",
    "\n",
    "    def dRelu(self, z):\n",
    "        # Derivative of sigmoid function\n",
    "        if z > 0: return 1\n",
    "        else: return 0\n",
    "        \n",
    "    def costFunction(self, X, y):\n",
    "        # Compute the Cost Function using weights already stored in clas\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        J = np.sum((y-self.yHat)**2) / X.shape[0] + (self.Lambda/2) * (np.sum(self.W1**2) + np.sum(self.W2**2))\n",
    "        return J     \n",
    "\n",
    "    def dCostFunction(self, X, y):\n",
    "        # Compute derivative with respect to W1 and W2\n",
    "        self.yHat = self.forward(X)\n",
    "\n",
    "        delta3 = np.multiply(-(y-self.yHat), self.dSigmoid(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3) / X.shape[0] + self.Lambda*self.W2\n",
    "\n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.dSigmoid(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2) / X.shape[0] + self.Lambda*self.W1\n",
    "\n",
    "        return dJdW1, dJdW2\n",
    "\n",
    "    # Helper functions for interacting with other classes\n",
    "    def getParameters(self):\n",
    "        # Get W1 and W2\n",
    "        parameters = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return parameters\n",
    "    \n",
    "    def setParameters(self, parameters):\n",
    "        # Update the weights W1 and W2. ''Note we are passing 1D vector, we'll need to reshape\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(parameters[W1_start:W1_end], (self.inputLayerSize, self.hiddenLayerSize))\n",
    "\n",
    "        W2_end = W1_end + self.hiddenLayerSize * self.outputLayerSize\n",
    "        self.W2 = np.reshape(parameters[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "\n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.dCostFunction(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN = Neural_Network_Regularized(Lambda=0.0001)\n",
    "T = trainer(NN)\n",
    "T.train(trainX, trainY, testX, testY, 200) # Pass data and epochs to the training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(T.J, 'b-', label='Train set Loss')\n",
    "plt.plot(T.testJ, 'r-', label='Test set Loss')\n",
    "plt.grid(1)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
