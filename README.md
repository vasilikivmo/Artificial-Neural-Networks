# Artificial Neural Networks - Demistified

Artificial Neural Networks are a Deep Learning Tool to solve tasks. They were born decades ago but their popularity has been increased exponentially since last 10 years, specially when Google opened TensorFlow for public use. However, there is something missing in this equation. A generation of Data Scientist has born, using these tools all together to solve their specific problems. To do so, the majority of them rely on those frameworks and 'the magic behind them'. [Andrej Kaparthy](http://karpathy.github.io) made a really good point in this [article](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b) in Medium.
In this work, there is a detailed explanation of this magic that is happening under the hood of the frameworks. We should all know what is going on and don't depend on how other people do things, because only that way we will solve the problems that, for sure, we will encounter.

The work is divided in 4 chapters to follow, where we will split into the smallest componentes our next best friend we can see here:

<img src="./Images/ANN Structure.png">

Please, find follow the different chapters in the next order:

[(1) - Neural Networks - First contact](https://github.com/PabloRR100/Artificial-Neural-Networks/blob/master/Documentation/1.%20Neural%20Networks%20-%20Basic%20Introduction.pdf)

[(2) - Neural Networks - The Big Picture](https://github.com/PabloRR100/Artificial-Neural-Networks/blob/master/Documentation/2.%20Neural%20Networks%20-%20The%20Big%20Picture.pdf)

[(3) - Neural Networks - The Graph Approach](https://github.com/PabloRR100/Artificial-Neural-Networks/blob/master/Documentation/3.%20Neural%20Networks%20-%20The%20Graph%20Approach.pdf)

[(4) - Neural Networks - Understanding BackProp Dimensionality](https://github.com/PabloRR100/Artificial-Neural-Networks/blob/master/Documentation/4.%20Neural%20Networks%20-%20Backpropagation%20Dimensionality%20Understanding.pdf)
